from google.colab import drive
drive.mount('/content/drive')
import numpy as np
import pandas as pd
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import MinMaxScaler
from keras.models import Model
from keras.layers import Dense, BatchNormalization, Input, Dropout
from keras.optimizers import Adam
from keras import regularizers
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score

# Load and preprocess data
df = pd.read_csv('/content/drive/My Drive/Data/ant-1.7.csv')
df[df.columns[-1]] = df[df.columns[-1]].apply(lambda x: 1 if x != 0 else 0)
target = df[df.columns[-1]]
features1 = df.drop(df.columns[-1], axis=1)
features = features1.drop(features1.columns[0:3], axis=1)

# Apply SMOTE to balance class distribution
smote = SMOTE(sampling_strategy=0.8, random_state=42)
x_resampled, y_resampled = smote.fit_resample(features, target)

# Apply Min-Max scaling to features
scaler = MinMaxScaler()
x_scaled = scaler.fit_transform(x_resampled)

# Initialize lists to store metric values across folds
accuracy_list = []
precision_list = []
recall_list = []
f1_list = []
average_feature_values = []

# Define autoencoder architecture
input_layer = Input(shape=x_scaled.shape[1:])
encoded = Dense(200, activation='tanh', activity_regularizer=regularizers.l1(10e-6))(input_layer)
encoded = BatchNormalization()(encoded)
encoded = Dense(150, activation='tanh')(encoded)
decoded = Dense(150, activation='tanh')(encoded)
decoded = BatchNormalization()(decoded)
decoded = Dense(200, activation='tanh')(decoded)
output_layer = Dense(x_scaled.shape[1], activation='sigmoid')(decoded)

autoencoder = Model(input_layer, output_layer)
autoencoder.compile(optimizer=Adam(lr=0.0001), loss="mse", metrics=['accuracy'])

# Stratified k-fold cross-validation
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
for fold, (train_index, test_index) in enumerate(kfold.split(x_scaled, y_resampled), 1):
    print(f"\nFold {fold}:")

    x_train_fold, x_test_fold = x_scaled[train_index], x_scaled[test_index]
    y_train_fold, y_test_fold = y_resampled[train_index], y_resampled[test_index]

    # Train autoencoder on entire training dataset
    autoencoder.fit(x_train_fold, x_train_fold, batch_size=30, epochs=200, shuffle=True, validation_split=0.20)

    # Create encoder model
    encoder = Model(inputs=input_layer, outputs=encoded)

    # Encode training and test data
    x_train_encoded = encoder.predict(x_train_fold)
    x_test_encoded = encoder.predict(x_test_fold)

    # Store average feature values for this fold
    average_feature_values.append(np.mean(x_train_encoded, axis=0))

    # Binary classification model
    classification_input = Input(shape=(x_train_encoded.shape[1],))
    classifier = Dense(64, activation='relu')(classification_input)
    classifier = BatchNormalization()(classifier)
    classifier = Dense(32, activation='relu')(classifier)
    classifier = BatchNormalization()(classifier)
    classifier = Dense(16, activation='relu')(classifier)
    classifier = BatchNormalization()(classifier)
    classifier = Dropout(0.2)(classifier)  # Added dropout layer
    classification_output = Dense(1, activation='sigmoid')(classifier)

    classification_model = Model(inputs=classification_input, outputs=classification_output)
    classification_model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])

    # Train classification model using encoded features
    classification_model.fit(x_train_encoded, y_train_fold, batch_size=15, epochs=100, shuffle=True)

    # Encode test data for classification
    x_test_encoded_for_classification = encoder.predict(x_test_fold)

    # ... (evaluate and print classification performance)
    y_pred_probabilities = classification_model.predict(x_test_encoded_for_classification)

    # Apply threshold to predict binary labels
    threshold = 0.6
    y_pred_binary = (y_pred_probabilities >= threshold).astype(int)

    # Calculate metrics for the current fold
    accuracy = accuracy_score(y_test_fold, y_pred_binary)
    precision = precision_score(y_test_fold, y_pred_binary)
    recall = recall_score(y_test_fold, y_pred_binary)
    f1 = f1_score(y_test_fold, y_pred_binary)

    # Append metric values to lists
    accuracy_list.append(accuracy)
    precision_list.append(precision)
    recall_list.append(recall)
    f1_list.append(f1)

    # Display metrics for each fold
    print(f"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}")

# Calculate and display average metrics across folds
avg_accuracy = np.mean(accuracy_list)
avg_precision = np.mean(precision_list)
avg_recall = np.mean(recall_list)
avg_f1 = np.mean(f1_list)

print("\nAverage Metrics Across Folds:")
print(f"Average Accuracy: {avg_accuracy:.4f}")
print(f"Average Precision: {avg_precision:.4f}")
print(f"Average Recall: {avg_recall:.4f}")
print(f"Average F1 Score: {avg_f1:.4f}")

# Calculate and display average feature values across folds
avg_feature_values = np.mean(average_feature_values, axis=0)
print("\nAverage Feature Values Across Folds:")
for i, avg_value in enumerate(avg_feature_values):
    print(f"Feature {i + 1}: {avg_value:.4f}")
